{"cells":[{"cell_type":"markdown","metadata":{"id":"q83iYaShSMXZ"},"source":["# Image Classification Pipeline"]},{"cell_type":"markdown","source":["This is an example of a fairly sophisticated Vertex AI pipeline for training and deploying an image classification model.\n","The focus lies on training and hyperparameter tuning, and not as much on deployment strategies. \n","\n","It is based on Keras, can use any base model from tfhub.dev and be applied to any dataset following some minor conventions.\n","\n","Running the whole pipeline **will incur costs** (**about 5â‚¬ per run with the current config**). Also, make sure to delete all created resources afterwards to avoid additional costs.\n"],"metadata":{"id":"dlEAkbIP5kV9"}},{"cell_type":"markdown","metadata":{"id":"dJt5bzX-SMXd"},"source":["## 1. Setup"]},{"cell_type":"markdown","source":["First we need to install all requirements and configure our environment properly."],"metadata":{"id":"VwXrhQe16VUm"}},{"cell_type":"markdown","metadata":{"id":"Mk7es0OYSMXe"},"source":["### 1.1. Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P06FnQ3SMXf"},"outputs":[],"source":["!pip install kfp==1.8.11\n","!pip instal google-cloud-aiplatform==1.9.0\n","!pip install google-cloud-pipeline-components==0.2.6\n","!pip install tensorflow==2.8.0\n","!pip install scikit-learn==1.0.2\n","!pip install cloudml-hypertune==0.1.0.dev6\n","!pip install tensorflow_hub==0.12.0\n"]},{"cell_type":"markdown","metadata":{"id":"eI04VaRmSMXg"},"source":["### 1.2. Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mz9d31FzSMXh"},"outputs":[],"source":["PROJECT = \"YOUR_PROJECT_ID\"\n","LOCATION = \"YOUR_REGION\"\n","SERVICE_ACCOUNT_NAME = \"vertex-pipeline-sa\"\n","\n","CONTAINER_REGISTRY_BASE = f\"eu.gcr.io/{PROJECT}/mlpipelines/vertex\"\n","\n","ARTIFACTS_ROOT_DIR = f\"gs://{PROJECT}_vertex-ai-pipelines_artifacts\"\n","COMPILED_JOBS_DIR = f\"gs://{PROJECT}_vertex-ai-pipelines_compiled-jobs\"\n","DATA_ROOT_DIR = f\"gs://{PROJECT}-data\"\n","MODEL_ROOT_DIR = f\"gs://{PROJECT}-model\"\n","TENSORBOARD_LOGS_DIR = f\"gs://{PROJECT}-tensorboard-logs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2AJbTMGSMXh"},"outputs":[],"source":["SERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_NAME}@{PROJECT}.iam.gserviceaccount.com\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8hPjaQsSMXi"},"outputs":[],"source":["TRAINING_IMAGE = f\"{CONTAINER_REGISTRY_BASE}/components/training:latest\"\n","SERVING_IMAGE = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest\""]},{"cell_type":"markdown","metadata":{"id":"Bz7F_ZxkSMXj"},"source":["### 1.3. Setup GCP Infrastructure\n","\n","Based on the above configurations, we setup the GCP infrastructure. This is for demonstration purposes only. In practice you should consider setting this up with a tool like terraform to make sure this setup is easily created and destroyed in a consistent and repeatable way.\n","\n","If you are on Colab, this will ask you for a login automatically. If you run this notebook on a Vertex workbench notebook, please configure gcloud beforehand.\n","\n","We are setting up:\n","\n","- activation of cloudbuild and aiplatform apis\n","- a service account to run the pipeline with on Vertex AI\n","- we add IAM roles for the service account\n","- we create different GCS buckets for pipeline artifacts, compiled pipeline jobs, data, pretrained models and tensorboard logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uITXGXCkSMXk"},"outputs":[],"source":["import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()\n","else:\n","    print(\"Run `gcloud auth login` in your terminal to authenticate\")"]},{"cell_type":"code","source":["if \"google.colab\" in sys.modules:\n","    !gcloud config set project $PROJECT\n"],"metadata":{"id":"v0PiH_m_UErW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trwZGqHkSMXm"},"outputs":[],"source":["!gcloud services enable aiplatform.googleapis.com\n","!gcloud services enable cloudbuild.googleapis.com\n","!gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME\n","!gcloud projects add-iam-policy-binding $PROJECT \\\n","    --member serviceAccount:$SERVICE_ACCOUNT \\\n","    --role=roles/aiplatform.admin\n","!gcloud projects add-iam-policy-binding $PROJECT \\\n","    --member serviceAccount:$SERVICE_ACCOUNT \\\n","    --role=roles/storage.admin\n","!gcloud projects add-iam-policy-binding $PROJECT \\\n","    --member serviceAccount:$SERVICE_ACCOUNT \\\n","    --role=roles/iam.serviceAccountUser\n","!gsutil mb $ARTIFACTS_ROOT_DIR\n","!gsutil mb $COMPILED_JOBS_DIR\n","!gsutil mb $DATA_ROOT_DIR\n","!gsutil mb $MODEL_ROOT_DIR\n","!gsutil mb $TENSORBOARD_LOGS_DIR"]},{"cell_type":"markdown","metadata":{"id":"95Z_fSQWSMXm"},"source":["### 1.4. Setup gcsfuse\n","\n","To more easily work in colab when it comes to interacting with GCS, we configure gcsfuse. In Colab be install gcsfuse first. On a Vertex AI workbench notebook, this will already be done, so we only configure a dynamic mount.\n","This gives you access to all gcs buckets you have permissions for. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwUfFNocSMXn"},"outputs":[],"source":["if \"google.colab\" in sys.modules:\n","    !echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n","    !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n","    !apt -qq update\n","    !apt -qq install gcsfuse "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"826LFd-uSMXn"},"outputs":[],"source":["!mkdir gcs\n","!gcsfuse --debug_gcs --debug_fuse --implicit-dirs gcs"]},{"cell_type":"markdown","metadata":{"id":"QCc_ldwxSMXo"},"source":["## 2. Pipeline Preparation"]},{"cell_type":"markdown","source":["Let's use the well known flower dataset as an example. \n","Here we download the data locally, extract it and copy it to gcs. We use gsutil, because it is faster for copying large amounts of data compared to relying on gcsfuse here.\n","\n","This dataset follows the exact convention this pipeline relies on. Under its root directory it has exactly one subdirectory per class, containing the respective images."],"metadata":{"id":"3rrOa1EE8lN3"}},{"cell_type":"markdown","metadata":{"id":"yt7oOxuOSMXo"},"source":["### 2.1. Prepare Data"]},{"cell_type":"code","source":["from pathlib import Path"],"metadata":{"id":"erYUClGtwFl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCwHsfSOSMXo"},"outputs":[],"source":["DATA_DIR_LOCAL = \"data/flower_data\"\n","DATA_DIR = f\"{DATA_ROOT_DIR}/flower_data\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfJ5jfDJSMXo"},"outputs":[],"source":["Path(DATA_DIR_LOCAL).mkdir(exist_ok=True)\n","Path(DATA_DIR.replace(\"gs://\", \"gcs/\")).mkdir(exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYXsn3vFSMXp"},"outputs":[],"source":["import urllib\n","DOWNLOAD_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n","urllib.request.urlretrieve(DOWNLOAD_URL, f\"{DATA_DIR_LOCAL}/flower_photos.tgz\")\n","!tar xfz $DATA_DIR_LOCAL/flower_photos.tgz -C $DATA_DIR_LOCAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulOjoREUSMXp"},"outputs":[],"source":["!gsutil -m cp -r $DATA_DIR_LOCAL $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"1rL-EItKSMXp"},"source":["### 2.2 Define Training Script"]},{"cell_type":"markdown","source":["Now we can move towards the more interesting part. \n","The goal of this section is to create a Docker image containing our training script, so we can use it later on. \n","\n","The name of the image is already defined in our configuration above. Here we create a training directory with which contains the source code for training, a Dockerfile and a requirements.txt file. \n","\n","The training script, together with the utils file, load the training dataset, splits it into train and validation sets, creates a model, configures Keras callbacks for writing logs, checkpoints and propagating your metrics to Vertex AI Hyperparameter-Tuning jobs.\n","\n","Note also the environment variables, which the training script relies upon. Those are provided by Vertex training jobs or the hyperparameter tuning jobs respectively."],"metadata":{"id":"v5WOOE3g9KGy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w11Lhs7vSMXp"},"outputs":[],"source":["!mkdir -p training/src"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGU2C7_wSMXq"},"outputs":[],"source":["%%writefile training/src/utils.py\n","import os\n","import pickle  # nosec\n","import shutil\n","from pathlib import Path\n","\n","import tensorflow as tf\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Dense, Dropout, Input, Resizing\n","from tensorflow.keras.models import Model\n","\n","import tensorflow_hub as hub\n","\n","import hypertune\n","\n","\n","class HyperTuneCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, metric=None) -> None:\n","        super().__init__()\n","        self.metric = metric\n","        self.hpt = hypertune.HyperTune()\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if logs and self.metric in logs:\n","            self.hpt.report_hyperparameter_tuning_metric(\n","                hyperparameter_metric_tag=self.metric,\n","                metric_value=logs[self.metric],\n","                global_step=epoch,\n","            )\n","\n","\n","def prepare_dataset(\n","    train_dataset_uri, compression, shuffle, shuffle_buffer, seed, batch_size, val_size\n","):\n","    with open(\n","        os.path.join(train_dataset_uri.replace(\"gs://\", \"/gcs/\"), \"element_spec.pickle\"), \"rb\"\n","    ) as fh:\n","        element_spec = pickle.load(fh)  # nosec\n","\n","    dataset = tf.data.experimental.load(\n","        train_dataset_uri, element_spec=element_spec, compression=compression\n","    )\n","\n","    if shuffle:\n","        dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n","\n","    dataset_size = dataset.cardinality().numpy()\n","    num_train_samples = (1.0 - val_size) * dataset_size\n","\n","    train_data, val_data = dataset.take(num_train_samples), dataset.skip(num_train_samples)\n","\n","    if shuffle:\n","        train_data = train_data.shuffle(shuffle_buffer, seed=seed)\n","        val_data = val_data.shuffle(shuffle_buffer, seed=seed)\n","\n","    train_data = train_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    val_data = val_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return train_data, val_data\n","\n","\n","def create_model(\n","    image_size, num_classes, base_model_dir, num_neurons, \n","    dropout, activation, learning_rate\n","):\n","    inputs = Input(shape=(None, None, 3))\n","    x = Resizing(*image_size)(inputs)\n","\n","    base_model = None\n","    if base_model_dir.startswith(\"gs://\"):\n","        base_model = tf.keras.models.load_model(base_model_dir.replace(\"gs://\", \"/gcs/\"))\n","\n","    if base_model_dir.startswith(\"https://tfhub.dev\"):\n","        base_model = hub.KerasLayer(\n","          base_model_dir, \n","          trainable=False, \n","          input_shape=(*image_size, 3)\n","        )\n","\n","    if not base_model:\n","        base_model = MobileNetV2(weights=\"imagenet\", input_tensor=x)\n","        base_model.trainable = False\n","        outputs = base_model.layers[-2].output\n","    else:\n","        outputs = base_model(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","\n","    outputs = Dense(num_neurons, activation=activation)(model.output)\n","    outputs = Dropout(dropout)(outputs)\n","    outputs = Dense(num_classes, activation=\"softmax\")(outputs)\n","    model = Model(inputs=model.input, outputs=outputs)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","    return model\n","\n","\n","def configure_keras_callbacks(\n","    tensorboard_log_dir,\n","    tensorboard_kwargs,\n","    checkpoints_dir,\n","    checkpoint_kwargs,\n","    hypertune,\n","    hypertune_kwargs,\n","):\n","    callbacks = []\n","\n","    if hypertune:\n","        callbacks.append(HyperTuneCallback(**hypertune_kwargs))\n","\n","    if tensorboard_log_dir:\n","        fuse_path = tensorboard_log_dir.replace(\"gs://\", \"/gcs/\")\n","        if Path(fuse_path).exists():\n","            shutil.rmtree(fuse_path)\n","        callbacks.append(\n","            tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir, **tensorboard_kwargs)\n","        )\n","    if checkpoints_dir:\n","        callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoints_dir, **checkpoint_kwargs))\n","\n","    return callbacks\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9D_3hwxSMXr"},"outputs":[],"source":["%%writefile training/src/trainer.py\n","\n","import argparse\n","import json\n","import os\n","\n","from utils import create_model, prepare_dataset, configure_keras_callbacks\n","import tensorflow as tf\n","\n","\n","def get_args():\n","    \"\"\"Parses args. Must include all hyperparameters you want to tune.\"\"\"\n","\n","    def custom_int(value):\n","        return int(float(value))\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--train-dataset-uri\", type=str)\n","    parser.add_argument(\"--num-classes\", type=custom_int)\n","    parser.add_argument(\"--compression\", type=str, default=\"NONE\")\n","    parser.add_argument(\"--base-model-dir\", type=str, default=None)\n","    parser.add_argument(\"--val-size\", type=float, default=0.2)\n","    parser.add_argument(\"--image-size\", type=json.loads, default=[124, 124])\n","    parser.add_argument(\"--epochs\", type=custom_int, default=1)\n","    parser.add_argument(\"--steps-per-epoch\", type=custom_int, default=100)\n","    parser.add_argument(\"--validation-steps\", type=custom_int, default=100)\n","    parser.add_argument(\"--batch-size\", type=custom_int, default=1)\n","    parser.add_argument(\"--shuffle\", type=bool, default=True)\n","    parser.add_argument(\"--shuffle-buffer\", type=custom_int, default=64)\n","    parser.add_argument(\"--seed\", type=custom_int, default=None)\n","    parser.add_argument(\"--save-checkpoints\", type=bool, default=False)\n","    parser.add_argument(\"--checkpoint-kwargs\", type=json.loads, default={})\n","    parser.add_argument(\"--tensorboard-log-root\", type=str, default=None)\n","    parser.add_argument(\"--tensorboard-kwargs\", type=json.loads, default={})\n","\n","    parser.add_argument(\"--hypertune\", type=bool, default=False)\n","\n","    # tunable parameters\n","    parser.add_argument(\"--num-neurons\", type=custom_int, default=64)\n","    parser.add_argument(\"--dropout\", type=float, default=0.5)\n","    parser.add_argument(\"--activation\", type=str, default=\"relu\")\n","    parser.add_argument(\"--learning-rate\", type=float, default=0.001)\n","\n","    args = parser.parse_args()\n","    return args\n","\n","\n","if __name__ == \"__main__\":\n","    args = get_args()\n","\n","    print(args)\n","    print(os.environ)\n","\n","    if args.tensorboard_log_root:\n","        job_subdir = os.environ.get(\"CLOUD_ML_JOB_ID\", \"\")\n","        trial_subdir = os.environ.get(\"CLOUD_ML_TRIAL_ID\", \"\")\n","        tensorboard_log_dir = os.path.join(\n","            args.tensorboard_log_root, job_subdir, trial_subdir\n","        ).rstrip(\"/\")\n","    else:\n","        tensorboard_log_dir = None\n","\n","    checkpoints_dir = os.environ.get(\"AIP_CHECKPOINT_DIR\") if args.save_checkpoints else None\n","\n","    trained_model_dir = os.environ.get(\"AIP_MODEL_DIR\")\n","\n","    print(tensorboard_log_dir, checkpoints_dir, trained_model_dir)\n","\n","    model = create_model(\n","        image_size=args.image_size,\n","        num_classes=args.num_classes,\n","        base_model_dir=args.base_model_dir,\n","        num_neurons=args.num_neurons,\n","        dropout=args.dropout,\n","        activation=args.activation,\n","        learning_rate=args.learning_rate,\n","    )\n","\n","    callbacks = configure_keras_callbacks(\n","        tensorboard_log_dir=tensorboard_log_dir,\n","        tensorboard_kwargs=args.tensorboard_kwargs,\n","        checkpoints_dir=checkpoints_dir,\n","        checkpoint_kwargs=args.checkpoint_kwargs,\n","        hypertune=args.hypertune,\n","        hypertune_kwargs=dict(metric=os.environ.get(\"CLOUD_ML_HP_METRIC_TAG\")),\n","    )\n","\n","    train_data, val_data = prepare_dataset(\n","        train_dataset_uri=args.train_dataset_uri,\n","        compression=args.compression,\n","        shuffle=args.shuffle,\n","        shuffle_buffer=args.shuffle_buffer,\n","        seed=args.seed,\n","        batch_size=args.batch_size,\n","        val_size=args.val_size,\n","    )\n","\n","    model.fit(\n","        train_data,\n","        epochs=args.epochs,\n","        steps_per_epoch=args.steps_per_epoch,\n","        validation_data=val_data,\n","        validation_steps=args.validation_steps,\n","        callbacks=callbacks,\n","    )\n","\n","    metric_values = model.evaluate(val_data)\n","\n","    metrics = {k: v for k, v in zip(model.metrics_names, metric_values)}\n","\n","    metadata = dict(framework=f\"Tensorflow {tf.__version__}\")\n","\n","    model.save(trained_model_dir)\n","\n","    trained_model_dir_fuse = trained_model_dir.replace(\"gs://\", \"/gcs/\")\n","\n","    with open(os.path.join(trained_model_dir_fuse, \"metrics.json\"), \"w\") as fh:\n","        json.dump(metrics, fh)\n","    with open(os.path.join(trained_model_dir_fuse, \"metadata.json\"), \"w\") as fh:\n","        json.dump(metadata, fh)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iaMXw5ySMXr"},"outputs":[],"source":["%%writefile training/requirements.txt\n","tensorflow==2.8\n","cloudml-hypertune==0.1.0.dev6\n","tensorflow_hub==0.12.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_SBcTSYSMXr"},"outputs":[],"source":["%%writefile training/Dockerfile\n","FROM python:3.8-slim\n","\n","COPY requirements.txt .\n","\n","RUN pip install -r requirements.txt --quiet --no-cache-dir \n","\n","COPY src /src"]},{"cell_type":"markdown","source":["Now let's build and push this image to the container registry."],"metadata":{"id":"bMpolrAVArvL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9xwD6cPSMXs"},"outputs":[],"source":["!gcloud builds submit training -t \"{CONTAINER_REGISTRY_BASE}/components/training:latest\""]},{"cell_type":"markdown","metadata":{"id":"tiu1yUp5SMXs"},"source":["### 2.2. Define Components"]},{"cell_type":"markdown","source":["Finally, we can define the Kubeflow pipeline components."],"metadata":{"id":"mx2HYcZoBDov"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp9hxZfgSMXs"},"outputs":[],"source":["from typing import NamedTuple\n","\n","from kfp.v2 import dsl\n","\n","from kfp.v2.dsl import Metrics, Model, Output, Artifact, Input, Dataset, ClassificationMetrics"]},{"cell_type":"markdown","metadata":{"id":"zA5M7BEQSMXs"},"source":["#### 2.2.1 Reusable Components"]},{"cell_type":"markdown","source":["We start with a few generally reusable components I created.\n","You can easily use them as they are in other pipelines."],"metadata":{"id":"meFUc8S0BKK3"}},{"cell_type":"markdown","metadata":{"id":"yI9SfZZUSMXs"},"source":["##### Get Worker Pool Spec"]},{"cell_type":"markdown","source":["This component takes a workerpool spec and adds arguments and environment variables to it, which can be provided as dicts."],"metadata":{"id":"mtN145CXBcks"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3HJVha-SMXs"},"outputs":[],"source":["@dsl.component(base_image=\"python:3.8-slim\")\n","def GetWorkerPoolSpecsOp(\n","    worker_pool_specs: list,\n","    args: dict = {},\n","    hyperparams: dict = {},\n","    env: dict = {},\n",") -> list:\n","\n","    for spec in worker_pool_specs:\n","        if \"args\" not in spec[\"container_spec\"]:\n","            spec[\"container_spec\"][\"args\"] = []\n","        for k, v in args.items():\n","            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n","        for k, v in hyperparams.items():\n","            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n","\n","        if env:\n","            if \"env\" not in spec[\"container_spec\"]:\n","                spec[\"container_spec\"][\"env\"] = []\n","            for k, v in env.items():\n","                spec[\"container_spec\"][\"env\"].append(dict(name=k, value=v))\n","\n","    return worker_pool_specs"]},{"cell_type":"markdown","metadata":{"id":"gIeFCIJISMXs"},"source":["##### Get Custom Job Results"]},{"cell_type":"markdown","source":["This one takes a CustomJob resource, get all the results that job has written to gcs, moves them to the artifacts bucket and outputs a model and a metric artifact.\n","\n","This only assumes that your training writes a metadata.json and a metrics.json file into the same directory you have written your model to in the training script. "],"metadata":{"id":"nWvaEUY5BsEn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyNWMQ86SMXt"},"outputs":[],"source":["@dsl.component(\n","    base_image=\"python:3.8-slim\",\n","    packages_to_install=[\n","        \"google-cloud-pipeline-components==0.2.6\",\n","        \"google-cloud-aiplatform==1.9.0\",\n","    ],\n",")\n","def GetCustomJobResultsOp(\n","    project: str,\n","    location: str,\n","    job_resource: str,\n","    model: Output[Model],\n","    metrics: Output[Metrics],\n","):\n","    import json\n","    import shutil\n","    from pathlib import Path\n","    import google.cloud.aiplatform as aip\n","    from google.protobuf.json_format import Parse\n","    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n","\n","    aip.init(project=project, location=location)\n","\n","    training_gcp_resources = Parse(job_resource, GcpResources())\n","    custom_job_id = training_gcp_resources.resources[0].resource_uri\n","    custom_job_name = custom_job_id[custom_job_id.find(\"project\"):]\n","\n","    job_resource = aip.CustomJob.get(custom_job_name).gca_resource\n","    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n","\n","    job_base_dir_fuse = job_base_dir.replace(\"gs://\", \"/gcs/\")\n","    model_uri_fuse = model.uri.replace(\"gs://\", \"/gcs/\")\n","\n","    shutil.copytree(job_base_dir_fuse, Path(model_uri_fuse).parent, dirs_exist_ok=True)\n","    shutil.rmtree(job_base_dir_fuse)\n","\n","    with open(f\"{model_uri_fuse}/metadata.json\") as fh:\n","        model_metadata = json.load(fh)\n","\n","    with open(f\"{model_uri_fuse}/metrics.json\") as fh:\n","        metrics_dict = json.load(fh)\n","\n","    for k, v in metrics_dict.items():\n","        metrics.log_metric(k, v)\n","\n","    model.metadata = model_metadata"]},{"cell_type":"markdown","metadata":{"id":"zBYMPT62SMXt"},"source":["##### Get Hyperparameter-Tuning Job Results"]},{"cell_type":"markdown","source":["This component does the corresponding thing (to the component above) for the hyperparameter tuning job.\n","It does however output a dict of the best hyperparameters based on the study spec metrics provided. "],"metadata":{"id":"Q4fQhyohCPKT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHcQCl_CSMXt"},"outputs":[],"source":["@dsl.component(\n","    base_image=\"python:3.8-slim\",\n","    packages_to_install=[\n","        \"google-cloud-pipeline-components==0.2.6\",\n","        \"google-cloud-aiplatform==1.9.0\",\n","    ],\n",")\n","def GetHyperparameterTuningJobResultsOp(\n","    project: str, location: str, job_resource: str, study_spec_metrics: list\n",") -> dict:\n","    import google.cloud.aiplatform as aip\n","    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n","    from google.protobuf.json_format import Parse\n","    from google.cloud.aiplatform_v1.types import study\n","\n","    aip.init(project=project, location=location)\n","\n","    gcp_resources_proto = Parse(job_resource, GcpResources())\n","    tuning_job_id = gcp_resources_proto.resources[0].resource_uri\n","    tuning_job_name = tuning_job_id[tuning_job_id.find(\"project\"):]\n","\n","    job_resource = aip.HyperparameterTuningJob.get(tuning_job_name).gca_resource\n","\n","    trials = job_resource.trials\n","\n","    if len(study_spec_metrics) > 1:\n","        raise RuntimeError(\n","            \"Unable to determine best parameters for multi-objective hyperparameter tuning.\"\n","        )\n","\n","    goal = study_spec_metrics[0][\"goal\"]\n","    if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n","        best_fn = max\n","    elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n","        best_fn = min\n","    best_trial = best_fn(trials, key=lambda trial: trial.final_measurement.metrics[0].value)\n","\n","    return {p.parameter_id: p.value for p in best_trial.parameters}"]},{"cell_type":"markdown","metadata":{"id":"Qx_dy3umSMXt"},"source":["#### Add Servig Config"]},{"cell_type":"markdown","source":["This component takes a general model artifact and adds a provided serving config to the metadata. This is required, because the model upload component provided by google expects this information there.  "],"metadata":{"id":"EK7DSKbqDZzK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0y7pOinSMXt"},"outputs":[],"source":["@dsl.component(base_image=\"python:3.8-slim\")\n","def AddServingConfigOp(\n","    trained_model: Input[Model],\n","    configured_model: Output[Artifact],\n","    serving_config: dict,\n","):\n","    configured_model.uri = trained_model.uri\n","    configured_model.metadata = trained_model.metadata\n","    configured_model.metadata.update(serving_config)"]},{"cell_type":"markdown","metadata":{"id":"AioajlH_SMXt"},"source":["#### 2.2.2. Pipeline Specific Components"]},{"cell_type":"markdown","source":["We can continue with the pipeline specific components from here. If you want to adapt this pipeline for other machine learning problems or add particular preprocessing steps, those are the one you have to replace. Next to the training script of course."],"metadata":{"id":"aUuXGhVKDsrb"}},{"cell_type":"markdown","metadata":{"id":"hC1_CQjBSMXt"},"source":["##### Data Processing"]},{"cell_type":"markdown","source":["Start by writing a preprocessing component, which takes the dataset, applies some processing, splits it and outputs a training and test dataset. "],"metadata":{"id":"TXH7MwvrEdsM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yhc0gkaJSMXt"},"outputs":[],"source":["@dsl.component(base_image=\"tensorflow/tensorflow:2.8.0\")\n","def PreprocessingOp(\n","    dataset: Input[Dataset],\n","    train_dataset: Output[Dataset],\n","    test_dataset: Output[Dataset],\n","    compression: str = \"GZIP\",\n","    test_size: float = 0.1,\n","    data_load_kwargs: dict = {},\n","    seed: int = None,\n",") -> NamedTuple(\"Outputs\", [(\"train_dataset_uri\", str), (\"test_dataset_uri\", str)]):\n","    import logging\n","    import os\n","    import pickle  # nosec\n","    from collections import namedtuple\n","\n","    import tensorflow as tf\n","\n","    logging.info(\"Start Processing.\")\n","\n","    data_dir_fuse = dataset.uri.replace(\"gs://\", \"/gcs/\")\n","\n","    train_dataset_dir_fuse = train_dataset.uri.replace(\"gs://\", \"/gcs/\")\n","    test_dataset_dir_fuse = test_dataset.uri.replace(\"gs://\", \"/gcs/\")\n","\n","    train_data = tf.keras.utils.image_dataset_from_directory(\n","        data_dir_fuse, subset=\"training\", validation_split=test_size, seed=seed, **data_load_kwargs\n","    )\n","    test_data = tf.keras.utils.image_dataset_from_directory(\n","        data_dir_fuse,\n","        subset=\"validation\",\n","        validation_split=test_size,\n","        seed=seed,\n","        **data_load_kwargs\n","    )\n","\n","    train_dataset.metadata[\"classes\"] = train_data.class_names\n","    test_dataset.metadata[\"classes\"] = test_data.class_names\n","\n","    train_data = train_data.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n","    test_data = test_data.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n","\n","    tf.data.experimental.save(dataset=train_data, path=train_dataset.uri, compression=compression)\n","    with open(os.path.join(train_dataset_dir_fuse, \"element_spec.pickle\"), \"wb\") as fh:\n","        pickle.dump(train_data.element_spec, fh)  # nosec\n","\n","    tf.data.experimental.save(dataset=test_data, path=test_dataset.uri, compression=compression)\n","    with open(os.path.join(test_dataset_dir_fuse, \"element_spec.pickle\"), \"wb\") as fh:\n","        pickle.dump(test_data.element_spec, fh)  # nosec\n","\n","    logging.info(\"Finished Processing.\")\n","\n","    output = namedtuple(\"Outputs\", [\"train_dataset_uri\", \"test_dataset_uri\"])\n","    return output(train_dataset_uri=train_dataset.uri, test_dataset_uri=test_dataset.uri)\n"]},{"cell_type":"markdown","metadata":{"id":"lB3_0FlZSMXu"},"source":["##### Evaluation"]},{"cell_type":"markdown","source":["In the evaluation component, we want to take the test dataset, the trained model and output some metrics. \n","This example shows how to compute some standard classification metrics and creates a confusion matrix. \n","\n","Finally it also outputs a string \"true\"/\"false\" (string because kubeflow requires strings for conditionals), which represents the decision whether or not to upload the model to the vertex model registry. This decision is made based on thresholds defined for different metrics and provided as a dict to this component. "],"metadata":{"id":"fM8O33G_GhFK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCLV_fAKSMXu"},"outputs":[],"source":["@dsl.component(base_image=\"tensorflow/tensorflow:2.8.0\", packages_to_install=[\"scikit-learn==1.0.2\"])\n","def EvaluateOp(\n","    test_dataset: Input[Dataset],\n","    trained_model: Input[Model],\n","    metrics: Output[Metrics],\n","    confusion_matrix: Output[ClassificationMetrics],\n","    batch_size: int = 16,\n","    compression: str = \"GZIP\",\n","    upload_thresholds: dict = {},\n",") -> NamedTuple(\"Outputs\", [(\"upload_decision\", str)]):\n","    import logging\n","    import os\n","    import pickle  # nosec\n","    from collections import namedtuple\n","\n","    import numpy as np\n","    import tensorflow as tf\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.metrics import confusion_matrix as conf_mat\n","    from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","    logging.info(\"Starting Evaluation.\")\n","\n","    model = tf.keras.models.load_model(trained_model.uri.replace(\"gs://\", \"/gcs/\"))\n","\n","    with open(\n","        os.path.join(test_dataset.uri.replace(\"gs://\", \"/gcs/\"), \"element_spec.pickle\"), \"rb\"\n","    ) as fh:\n","        element_spec = pickle.load(fh)  # nosec\n","\n","    dataset = tf.data.experimental.load(\n","        test_dataset.uri, element_spec=element_spec, compression=compression\n","    )\n","\n","    test_data = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","    y_pred_probs = model.predict(test_data)\n","    y_pred = y_pred_probs.argmax(axis=1)\n","    y_true = np.concatenate([y for _, y in test_data], axis=0)\n","\n","    metrics.log_metric(\"accuracy\", accuracy_score(y_true=y_true, y_pred=y_pred))\n","    metrics.log_metric(\"precision\", precision_score(y_true=y_true, y_pred=y_pred, average=\"macro\"))\n","    metrics.log_metric(\"recall\", recall_score(y_true=y_true, y_pred=y_pred, average=\"macro\"))\n","    metrics.log_metric(\"f1\", f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"))\n","\n","    confusion = conf_mat(y_true=y_true, y_pred=y_pred)\n","\n","    confusion_matrix.log_confusion_matrix(\n","        categories=test_dataset.metadata[\"classes\"], matrix=confusion.tolist()\n","    )\n","\n","    logging.info(\"Evaluation completed.\")\n","\n","    decision = True\n","    for metric, value in metrics.metadata.items():\n","        required = upload_thresholds.get(metric, 0)\n","        if value < required:\n","            decision = False\n","\n","    output = namedtuple(\"Outputs\", [\"upload_decision\"])\n","    return output(upload_decision=\"true\" if decision else \"false\")\n"]},{"cell_type":"markdown","metadata":{"id":"2WjB7rdhSMXu"},"source":["##### Training Args"]},{"cell_type":"markdown","source":["This is a minor component that is required because kubeflow has a limitation when it comes to providing dict inputs to components, which contain pipeline parameters (also outputs form previous components). So we create a component that takes all the training arguments, and returns them in a dict. Note that dicts in the dict need to be serialized explicitly."],"metadata":{"id":"sDtOAGuaHqSa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqQvLo5mSMXu"},"outputs":[],"source":["@dsl.component(base_image=\"python:3.8-slim\")\n","def GetTrainingArgsDictOp(\n","    base_model_dir: str,\n","    train_dataset: Input[Dataset],\n","    compression: str,\n","    val_size: float,\n","    image_size: list,\n","    epochs: int,\n","    steps_per_epoch: int,\n","    validation_steps: int,\n","    batch_size: int,\n","    shuffle: bool,\n","    shuffle_buffer: int,\n","    seed: int,\n","    save_checkpoints: bool,\n","    checkpoint_kwargs: dict,\n","    tensorboard_log_root: str,\n","    tensorboard_kwargs: dict,\n","    hypertune: bool,\n",") -> dict:\n","    import json\n","\n","    train_dataset_uri = train_dataset.uri\n","    num_classes = len(train_dataset.metadata[\"classes\"])\n","\n","    return dict(\n","        base_model_dir=base_model_dir,\n","        train_dataset_uri=train_dataset_uri,\n","        num_classes=num_classes,\n","        compression=compression,\n","        val_size=val_size,\n","        image_size=image_size,\n","        epochs=epochs,\n","        steps_per_epoch=steps_per_epoch,\n","        validation_steps=validation_steps,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        shuffle_buffer=shuffle_buffer,\n","        seed=seed,\n","        save_checkpoints=save_checkpoints,\n","        checkpoint_kwargs=json.dumps(checkpoint_kwargs),\n","        tensorboard_log_root=tensorboard_log_root,\n","        tensorboard_kwargs=json.dumps(tensorboard_kwargs),\n","        hypertune=hypertune,\n","    )\n"]},{"cell_type":"markdown","metadata":{"id":"cdoqdOQ4SMXu"},"source":["## 3. Pipeline Definition"]},{"cell_type":"markdown","source":["Finally we can compose the created components into a pipeline. "],"metadata":{"id":"lKzFxmBlIGzE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mbo8QdfSMXu"},"outputs":[],"source":["from google_cloud_pipeline_components.experimental import custom_job\n","from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n","import google_cloud_pipeline_components.aiplatform as gcc_aip\n","from google.cloud.aiplatform import hyperparameter_tuning as hpt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8fFz5QzSMXu"},"outputs":[],"source":["# define the workerpool spec for the custom jobs \n","# (https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec)\n","WORKER_POOL_SPECS = [\n","    dict(\n","        machine_spec=dict(\n","            machine_type=\"n1-standard-4\",\n","        ),\n","        replica_count=1,\n","        container_spec=dict(\n","            image_uri=TRAINING_IMAGE,\n","            command=[\"python\", \"/src/trainer.py\"],\n","        ),\n","    )\n","]\n","\n","# define the metric spec for hyperparameter tuning\n","# for details: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#MetricSpec\n","METRIC_SPEC = dict(val_accuracy=\"maximize\")\n","\n","# define the parameter specs for tuning\n","# for details: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#ParameterSpec\n","PARAMETER_SPEC = {\n","    \"learning-rate\": hpt.DoubleParameterSpec(min=0.0001, max=1, scale=\"log\"),\n","    \"activation\": hpt.CategoricalParameterSpec(values=[\"swish\", \"relu\", \"elu\"]),\n","    \"num-neurons\": hpt.DiscreteParameterSpec(values=[64, 128, 512], scale=None),\n","    \"dropout\": hpt.DiscreteParameterSpec(values=[0.3, 0.4, 0.5, 0.6], scale=None),\n","}\n","\n","# create the serving config\n","# detals: https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/types/artifact_types.py\n","# container spec: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ModelContainerSpec\n","# predict schema: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/PredictSchemata\n","SERVING_CONFIG = dict(containerSpec=dict(imageUri=SERVING_IMAGE))\n","\n","\n","PIPELINE_NAME = \"classifier-tuning-training-pipeline\"\n","\n","\n","@dsl.pipeline(\n","    name=PIPELINE_NAME,\n","    description=\"A training pipeline an image classifier.\",\n",")\n","def pipeline(\n","    project: str = PROJECT,\n","    location: str = LOCATION,\n","    base_model_dir: str = MODEL_ROOT_DIR,\n","    dataset_dir: str = DATA_ROOT_DIR,\n","    compression: str = \"GZIP\",\n","    deploy: str = \"false\",\n","    seed: int = 1,\n","):\n","\n","    # loads dataset\n","    import_dataset_step = dsl.importer(\n","        artifact_uri=dataset_dir,\n","        artifact_class=dsl.Dataset,\n","        reimport=False,\n","    ).set_display_name(\"Load-Dataset\")\n","\n","    # preprocesses data\n","    preprocessing_step = PreprocessingOp(\n","        dataset=import_dataset_step.output,\n","        compression=compression,\n","        test_size=0.1,\n","        data_load_kwargs=dict(\n","            labels=\"inferred\",\n","            label_mode=\"int\",\n","            color_mode=\"rgb\",\n","            image_size=[224, 224],\n","            batch_size=6,\n","            shuffle=True,\n","        ),\n","        seed=seed,\n","    ).set_display_name(\"Process-Data\")\n","\n","    # define training args for tuning (hypertune = True and e.g. less epochs)\n","    args = dict(\n","        base_model_dir=base_model_dir,\n","        train_dataset=preprocessing_step.outputs[\"train_dataset\"],\n","        compression=compression,\n","        val_size=0.2,\n","        image_size=[224, 224],\n","        epochs=5,\n","        steps_per_epoch=100,\n","        validation_steps=100,\n","        batch_size=3,\n","        shuffle=True,\n","        shuffle_buffer=64,\n","        seed=seed,\n","        save_checkpoints=True,\n","        checkpoint_kwargs=dict(monitor=\"val_accuracy\", mode=\"max\", save_best_only=True),\n","        tensorboard_log_root=f\"{TENSORBOARD_LOGS_DIR}/{PIPELINE_NAME}\",\n","        tensorboard_kwargs=dict(),\n","        hypertune=True,\n","    )\n","\n","    # create the args dict to pass to next component\n","    hypertune_args_step = GetTrainingArgsDictOp(**args).set_display_name(\"Get-Hypertune-Args\")\n","\n","    # create the workerpool spec for hyperparameter tuning\n","    # dont provide hyperparams, because they are defined in the PARAMETER_SPEC\n","    # and directly passed to the hyperparameter tuning job\n","    hypertune_worker_pool_specs_step = GetWorkerPoolSpecsOp(\n","        worker_pool_specs=WORKER_POOL_SPECS,\n","        args=hypertune_args_step.output,\n","    ).set_display_name(\"Get-Hypertune-Worker-Pool-Spec\")\n","\n","    # create the actual hyperparameter tuning job\n","    # here you can choose how many trials to do and how many to run in parallel\n","    hypertune_step = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n","        display_name=\"hypertune-job\",\n","        project=project,\n","        location=location,\n","        service_account=SERVICE_ACCOUNT,\n","        worker_pool_specs=hypertune_worker_pool_specs_step.output,\n","        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(METRIC_SPEC),\n","        study_spec_parameters=hyperparameter_tuning_job.utils.serialize_parameters(PARAMETER_SPEC),\n","        max_trial_count=6,\n","        parallel_trial_count=2,\n","        base_output_directory=f\"{ARTIFACTS_ROOT_DIR}/{PIPELINE_NAME}/hypertune-job\"\n","    ).set_display_name(\"Hypertune-Job\")\n","\n","    # now we can extract the results of the hyperparameter tuning job\n","    hypertune_results_step = GetHyperparameterTuningJobResultsOp(\n","        project=project,\n","        location=location,\n","        job_resource=hypertune_step.output,\n","        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(METRIC_SPEC),\n","    ).set_display_name(\"Get-Hypertune-Results\")\n","\n","    # update our args dict for training\n","    args.update(dict(hypertune=False))\n","\n","    # create the args dict again\n","    training_args_step = GetTrainingArgsDictOp(**args).set_display_name(\"Get-Training-Args\")\n","\n","    # create the workerpool spec for final training\n","    # this time we provide the hyperparams form the tuning job results\n","    training_worker_pool_specs_step = GetWorkerPoolSpecsOp(\n","        worker_pool_specs=WORKER_POOL_SPECS,\n","        hyperparams=hypertune_results_step.output,\n","        args=training_args_step.output,\n","    ).set_display_name(\"Get-Training-Worker-Pool-Spec\")\n","\n","    # here we run the final training job\n","    training_step = custom_job.CustomTrainingJobOp(\n","        display_name=\"training-job\",\n","        project=project,\n","        location=location,\n","        service_account=SERVICE_ACCOUNT,\n","        worker_pool_specs=training_worker_pool_specs_step.output,\n","        base_output_directory=f\"{ARTIFACTS_ROOT_DIR}/{PIPELINE_NAME}/training-job\",\n","        labels=dict(),\n","    ).set_display_name(\"Training-Job\")\n","\n","    # now we can extract the training results\n","    training_results_step = GetCustomJobResultsOp(\n","        project=project, location=location, job_resource=training_step.output\n","    ).set_display_name(\"Get-Training-Results\")\n","\n","    # we evaluate the model performance\n","    # prove thresholds for all the metric you compute in the evaluation component\n","    evaluation_step = EvaluateOp(\n","        test_dataset=preprocessing_step.outputs[\"test_dataset\"],\n","        trained_model=training_results_step.outputs[\"model\"],\n","        compression=compression,\n","        batch_size=2,\n","        upload_thresholds=dict(accuracy=0.90, precision=0.90, recall=0.90),\n","    ).set_display_name(\"Evaluation\")\n","\n","    # the result of the evaluation is a decision about model uploading\n","    with dsl.Condition(evaluation_step.outputs[\"upload_decision\"] == \"true\"):\n","\n","        # we add the serving config to the trained model artifact\n","        add_serving_config_step = AddServingConfigOp(\n","            trained_model=training_results_step.outputs[\"model\"],\n","            serving_config=SERVING_CONFIG,\n","        ).set_display_name(\"Add-Serving-Config\")\n","\n","        # upload the model to the Vertex model registry\n","        upload_model_step = gcc_aip.ModelUploadOp(\n","            project=project,\n","            location=location,\n","            display_name=\"rps-classifier\",\n","            unmanaged_container_model=add_serving_config_step.outputs[\"configured_model\"],\n","        ).set_display_name(\"Upload-Model\")\n","\n","        # again a condition, base on the pipeline parameter \"deploy\"\n","        # only deploy if this parameter it true\n","        with dsl.Condition(deploy == \"true\"):\n","            # creates an endpoint to deploy model to\n","            endpoint_create_step = gcc_aip.EndpointCreateOp(\n","                project=project, location=location, display_name=\"classifier-endpoint\"\n","            ).set_display_name(\"Create-Deployment-Endpoint\")\n","            \n","            # deploys model to endpoint\n","            gcc_aip.ModelDeployOp(\n","                model=upload_model_step.outputs[\"model\"],\n","                endpoint=endpoint_create_step.outputs[\"endpoint\"],\n","                dedicated_resources_machine_type=\"n1-standard-4\",\n","                dedicated_resources_min_replica_count=1,\n","            ).set_display_name(\"Deploy-Model\")"]},{"cell_type":"markdown","metadata":{"id":"C_GuwhXASMXv"},"source":["## 4. Compile and Run Pipeline"]},{"cell_type":"markdown","source":["The last step is to compile the pipeline and run it on Vertex AI pipelines.\n","\n","I also open up tensorboard directly, pointing to the tensorboard logs bucket. This will actually allow you to monitor the hyperparameter tuning jobs and the final training job."],"metadata":{"id":"1WsGW2d2ITXv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"23SgaGl8SMXv"},"outputs":[],"source":["from kfp.v2 import compiler\n","from google.cloud import aiplatform as aip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F84oCAZ5SMXv"},"outputs":[],"source":["COMPILED_JOB_PATH = f\"{COMPILED_JOBS_DIR.replace('gs://', 'gcs/')}/{PIPELINE_NAME}/{PIPELINE_NAME}.json\""]},{"cell_type":"code","source":["Path(COMPILED_JOB_PATH).parent.mkdir(exist_ok=True)"],"metadata":{"id":"uAhu1rinvhmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt2Cd9ZPSMXv"},"outputs":[],"source":["# overwrite pipeline parameters\n","pipeline_parameters = dict(\n","    project=PROJECT,\n","    location=LOCATION,\n","    base_model_dir=\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/5\",\n","    dataset_dir=f\"{DATA_ROOT_DIR}/flower_data/flower_photos\",\n","    compression=\"GZIP\",\n","    deploy=\"true\",\n","    seed=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN9DTltPSMXv"},"outputs":[],"source":["compiler.Compiler().compile(\n","            pipeline_func=pipeline,\n","            package_path=COMPILED_JOB_PATH,\n","            pipeline_parameters=pipeline_parameters,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mj8QvGpWSMXw"},"outputs":[],"source":["aip.init(project=PROJECT, location=LOCATION)\n","\n","pipeline_job = aip.PipelineJob(\n","    display_name=PIPELINE_NAME,\n","    template_path=COMPILED_JOB_PATH,\n","    pipeline_root=f\"{ARTIFACTS_ROOT_DIR}/{PIPELINE_NAME}\",\n","    enable_caching=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjGe4-1KSMXw"},"outputs":[],"source":["pipeline_job.submit(service_account=SERVICE_ACCOUNT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC_7s861SMXw"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z23zHvfNSMXw"},"outputs":[],"source":["%tensorboard --logdir $TENSORBOARD_LOGS_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnErYjepSMXw"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[{"file_id":"1q1mBaQIQKQ_1ephZN9Rs6JmAaIf5Ga4X","timestamp":1669212470959}]}},"nbformat":4,"nbformat_minor":0}